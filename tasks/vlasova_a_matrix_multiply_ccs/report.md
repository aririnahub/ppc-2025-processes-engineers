# Умножение разреженных матриц в формате CCS с использованием MPI
Студент: Власова Арина Сергеевна, группа 3823Б1ПР2
Технология: SEQ | MPI 
Вариант: 5

## 1. Введение
Операция умножения разреженных матриц является фундаментальной в вычислительной математике и находит применение в научных расчётах, машинном обучении и обработке графов. Использование формата CCS (Column Compressed Storage) позволяет эффективно хранить и обрабатывать матрицы с большим количеством нулевых элементов. Цель данной работы — реализация параллельного алгоритма умножения таких матриц с применением MPI для распределения вычислений между несколькими процессами, что должно обеспечить существенное ускорение по сравнению с последовательной версией.

## 2. Постановка задачи
Формальная задача: Умножение двух разреженных матриц A (размер m × k) и B (размер k × n), представленных в формате CCS. Результат — матрица C = A × B того же формата.

Входные данные: Пара матриц в формате CCS (три массива: values, row_indices, col_ptrs, а также размеры rows, cols, nnz).
Выходные данные: Результирующая матрица C в формате CCS.
Ограничения: Количество столбцов матрицы A должно совпадать с количеством строк матрицы B (k). Элементы матриц имеют тип double.

## 3. Последовательный алгоритм 
Базовый последовательный алгоритм состоит из двух основных шагов:
1) Транспонирование матрицы A: Для эффективного доступа к строкам A при умножении матрица транспонируется. Алгоритм транспонирования:
- Подсчитывается количество ненулевых элементов в каждой строке A (после транспонирования — в каждом столбце Aᵀ).
- На основе этих подсчётов строится массив указателей col_ptrs для Aᵀ.
- Элементы перераспределяются в новые массивы values и row_indices с сохранением порядка по столбцам.

2) Умножение Aᵀ на B:
- Для каждого столбца j матрицы B создаётся временный массив-накопитель temp_row размера m (число строк A), инициализированный нулями, и массив-маркер row_marker.
- Для каждого ненулевого элемента b[row_b][j] находится соответствующая строка row_b в Aᵀ.
- Каждый элемент этой строки Aᵀ умножается на b[row_b][j], и результат добавляется в temp_row на позиции, соответствующей столбцу исходной матрицы A.
- После обработки всех ненулевых элементов столбца B, из temp_row в результирующую матрицу C переносятся только значения, превышающие порог kEpsilon (1e-10).

## 4. Схема распараллеливания (MPI)
Распределение данных: Столбцы матрицы B распределяются между процессами MPI. Функция SplitColumns делит общее число столбцов B (n) на примерно равные части с учётом остатка. Каждый процесс получает свой непрерывный диапазон столбцов.

Роли процессов:
- Ранг 0 (координатор): Выполняет транспонирование матрицы A. Затем с помощью MPI_Bcast рассылает полученную транспонированную матрицу Aᵀ (её размеры и все три массива) всем остальным процессам. После этого выполняет локальное умножение для своего набора столбцов. В завершении собирает частичные результаты от всех процессов и объединяет их в итоговую матрицу C.

- Рабочие ранги (1..size-1): Получают матрицу Aᵀ, извлекают свои локальные столбцы из B с помощью ExtractLocalColumns и выполняют умножение. Отправляют свои результаты (массивы values, row_indices и col_ptrs для своей части столбцов) рангу 0 с использованием MPI_Send.

Коммуникационная модель: Используется однородная модель с ранговой рассылкой (broadcast) для Aᵀ и точечной коммуникацией (send/receive) для сбора результатов. Синхронизация в конце этапа выполняется с помощью MPI_Barrier.

## 5. Детали реализации
Структура кода: Проект разделён на логические модули. Общая структура данных SparseMatrixCCS и утилиты объявлены в common.hpp. Последовательная реализация алгоритма находится в ops_seq.hpp/cpp. MPI-версия, содержащая всю логику распараллеливания и коммуникации, реализована в ops_mpi.hpp/cpp.

Ключевые функции:
- TransposeMatrixMPI – транспонирование с предварительным подсчётом.
- ExtractLocalColumns – извлечение подмассива столбцов из матрицы B.
- MultiplyLocalMatrices и ProcessLocalColumn – ядро вычислений.
- ProcessRootRank и ProcessWorkerRank – управление логикой процессов.

Важные аспекты:
- Для экономии памяти и избежания лишних копий при сборе результатов используется семантика перемещения (std::move).
- Временные массивы temp_row и row_marker переиспользуются для каждого столбца.
- Элементы результата, по модулю меньшие kEpsilon, отбрасываются, что контролирует рост заполненности из-за ошибок округления.

## 6. Экспериментальная установка
### Аппаратное обеспечение:
- Процессор: Intel(R) Core(TM) Ultra 9 285H (2.90 GHz)
- Оперативная память: 32 GB
- ОС: Windows 11

### Инструментарий:
- Компилятор: clang version 21.1.6
- Версия MPI: MS-MPI v10.1.3
- Тип сборки: Release

## 7. Результаты и обсуждение
7.1 Корректность
Корректность параллельной реализации проверялась сравнением её результатов с результатами последовательного алгоритма для широкого набора сгенерированных матриц. Дополнительно проверялись структурные инварианты результирующей матрицы C: соответствие размеров (C.rows == A.rows, C.cols == B.cols), корректность размера массива col_ptrs (равен cols + 1) и монотонность его значений. Все функциональные тесты были пройдены успешно.

7.2 Производительность
Измерения проводились для матриц размером 2000x2000. Результаты представлены в таблице:

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 1.71    | 1.00    | N/A        |
| mpi         | 2     | 1.26    | 1.35    | 67.5%      |
| mpi         | 4     | 1.07    | 1.60    | 40.0%      |
| mpi         | 8     | 1.21    | 1.41    | 17.6%      |

Анализ: 
Алгоритм демонстрирует умеренное ускорение при увеличении числа процессов до 4. Дальнейший рост количества процессов приводит к снижению эффективности из-за увеличения накладных расходов на коммуникацию (рассылка Aᵀ и сбор результатов) и возможного дисбаланса нагрузки при неравномерном распределении ненулевых элементов по столбцам B.

## 8. Выводы
В ходе работы был успешно реализован и протестирован параллельный алгоритм умножения разреженных матриц в формате CCS с использованием MPI. Основным ограничением масштабируемости являются коммуникационные издержки. Для дальнейшего улучшения производительности можно рассмотреть более сбалансированную схему распределения столбцов (не по количеству, а по числу ненулевых элементов) и оптимизацию этапа сбора данных через использование коллективных операций MPI, таких как Gatherv.

## 9. Ссылки
1. Курс лекций по параллельному программированию Сысоева Александра Владимировича. 
2. Документация по курсу: https://learning-process.github.io/parallel_programming_course/ru

Приложение
```cpp
    bool VlasovaAMatrixMultiplyMPI::RunImpl() {
    const auto &[a, b] = GetInput();

    int rank = 0;
    int size = 0;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    SparseMatrixCCS at;
    if (rank == 0) {
        TransposeMatrixMPI(a, at);
    } else {
        at.rows = a.cols;
        at.cols = a.rows;
    }

    MPI_Bcast(&at.rows, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&at.cols, 1, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        at.nnz = static_cast<int>(at.values.size());
    }
    MPI_Bcast(&at.nnz, 1, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank != 0) {
        at.values.resize(at.nnz);
        at.row_indices.resize(at.nnz);
        at.col_ptrs.resize(at.cols + 1);
    }

    MPI_Bcast(at.values.data(), at.nnz, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    MPI_Bcast(at.row_indices.data(), at.nnz, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(at.col_ptrs.data(), at.cols + 1, MPI_INT, 0, MPI_COMM_WORLD);

    auto [start_col, end_col] = SplitColumns(b.cols, rank, size);
    int loc_cols = end_col - start_col;

    std::vector<double> loc_b_val;
    std::vector<int> loc_b_row_ind;
    std::vector<int> loc_b_col_ptr;

    ExtractLocalColumns(b, start_col, end_col, loc_b_val, loc_b_row_ind, loc_b_col_ptr);

    std::vector<double> loc_res_val;
    std::vector<int> loc_res_row_ind;
    std::vector<int> loc_res_col_ptr;

    MultiplyLocalMatrices(at, loc_b_val, loc_b_row_ind, loc_b_col_ptr, loc_cols, loc_res_val, loc_res_row_ind,
                            loc_res_col_ptr);

    if (rank == 0) {
        return ProcessRootRank(a, b, loc_res_val, loc_res_row_ind, loc_res_col_ptr, size);
    }

    return ProcessWorkerRank(loc_res_val, loc_res_row_ind, loc_res_col_ptr, loc_cols);
    }
```